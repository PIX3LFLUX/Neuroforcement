{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "combined-overall",
   "metadata": {},
   "source": [
    "# Examples\n",
    "This Notebook includes examples of the use of the Framework. This along with the READ ME on the Github Repository should give you a good introduction into the Framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a3fcd-fe05-45a2-b0bd-aba0d7b4c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install neat-python #make sure to run this everytime you restart your server if you would like to use NEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c099600-0afc-4ab8-bc7a-ecdc0140b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time, pickle, os, math, random, torch, neat\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from Modules import Visualization as Vis\n",
    "from Modules import RL_Environments\n",
    "from Modules import Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-failing",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-crisis",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Simple Toy Text Example - Frozen Lake\n",
    "In this simple example the agent can move left, right, up or down in a little grid world where the aim is to reach the other side of frozen lake.\n",
    "\n",
    "(Source: [Frozen Lake Gymnasium](https://gymnasium.farama.org/environments/toy_text/frozen_lake/))\\\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "* SFFF       (S: starting point, safe)\n",
    "* FHFH       (F: frozen surface, safe)\n",
    "* FFFH       (H: hole, episode termination)\n",
    "* HFFG       (G: goal, episode termination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-literacy",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating a Q-Table and filling it with Q-Values (Training the Agent)\n",
    "Training Loop to find the Q-Values and save them in a Q-Table to be used in future to traverse the frozen lake.\n",
    "\n",
    "If you want to train the Agent quickly without having the actions displayed each time comment out following statements:\n",
    "* `clear_output(wait=True)`\n",
    "* `print(\"*** Episode: \", episode)`\n",
    "* `Environment.env.render()`\n",
    "* `time.sleep(0.1)`\n",
    "\n",
    "And in the if done loop:\n",
    "* `print(\"*** Total Reward Received: \", reward)`\n",
    "* `time.sleep(1)`\n",
    "\n",
    "Play around with the settings for $\\epsilon$, $\\gamma$, learning rate, ...\n",
    "Onces you get decent results change the variable is_slippery to True and see what happens. \n",
    "What can be the reason that it's that the performance drops even if you increase the number of total episodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 1000\n",
    "max_steps = 500\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode=\"ansi\")\n",
    "Agent = Agents.QlearningAgent(env.observation_space.n, env.action_space.n, epsilon = 0.7, lr_rate = 0.81, gamma = 0.96)\n",
    "\n",
    "Reward_plot = Vis.Plotting()\n",
    "\n",
    "# Loop for each Episode\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    t = 0\n",
    "    \n",
    "    # Loop for each action, if the done flag is set, break\n",
    "    while t < max_steps:\n",
    "        #clear_output(wait=True)\n",
    "        #print(\"*** Episode: \", episode)\n",
    "        #env.render()\n",
    "        #time.sleep(0.1)\n",
    "        action = Agent.choose_action(state)  \n",
    "        if action == \"random\":\n",
    "            action = env.action_space.sample()\n",
    "        state2, reward, terminated, truncated, info = env.step(action)\n",
    "        #print('State: ', state, ' Action: ', action, ' State2: ', state2, ' Reward: ', reward)\n",
    "        Agent.learn(state, state2, reward, action)\n",
    "\n",
    "        state = state2\n",
    "        t += 1\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            Reward_plot.add_data(episode, reward)\n",
    "            #print(\"*** Total Reward Received: \", reward)`\n",
    "            #time.sleep(1)`\n",
    "            break\n",
    "            \n",
    "Reward_plot.plot_reward()\n",
    "\n",
    "print(\"Agent Q-Table\")\n",
    "print(Agent.Q)\n",
    "\n",
    "with open(\"Training/FrozenLakeQ.pkl\", 'wb') as f:\n",
    "    pickle.dump(Agent.Q, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-proof",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using the Training to move over the frozen lake\n",
    "This uses the previously determined Q-Table to traverse the frozen lake. So here the Agent does not explore the environment anymore, but just follows the max values in the Q-Table from each given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode=\"ansi\")\n",
    "\n",
    "with open(\"Training/FrozenLakeQ.pkl\", 'rb') as f:\n",
    "    Q = pickle.load(f)\n",
    "\n",
    "def choose_action(state):\n",
    "    action = np.argmax(Q[state, :])\n",
    "    return action\n",
    "\n",
    "# Loop for each episode\n",
    "for episode in range(5):\n",
    "\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    t = 0\n",
    "    \n",
    "    # Loop for each action, if the done flag is set, break\n",
    "    while t < 400:\n",
    "        clear_output(wait=True)\n",
    "        print(\"*** Episode: \", episode)\n",
    "        a = env.render()\n",
    "        print(a)\n",
    "\n",
    "        action = choose_action(state)\n",
    "\n",
    "        state2, reward, terminated, truncated, info = env.step(action) \n",
    "        state = state2\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            print(\"*** Total Reward Received: \", reward)\n",
    "            time.sleep(1)\n",
    "            break\n",
    "        \n",
    "        time.sleep(0.2)\n",
    "        os.system('clear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-conclusion",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-authorization",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## More Complex Toy Text Example - Taxi\n",
    "(Source: [Taxi Gymnasium](https://gymnasium.farama.org/environments/toy_text/taxi/))\\\n",
    "There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "\n",
    "The Taxi needs to pickup the passenger at the blue letter and drop off at the pink letter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-satin",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Loop\n",
    "Training Loop to find the Q-Values and save them in a Q-Table to be used to Play the Taxi game.\n",
    "\n",
    "If you want to train the Agent quickly without having the actions displayed each time comment out following statements:\n",
    "* `clear_output(wait=True)`\n",
    "* `print(\"*** Episode: \", episode)`\n",
    "* `Environment.env.render()`\n",
    "* `time.sleep(0.05)`\n",
    "\n",
    "And in the if done loop:\n",
    "* `print(\"*** Total Reward Received: \", reward)`\n",
    "* `time.sleep(1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_reward = 0\n",
    "average_reward = 0\n",
    "Reward_plot = Vis.Plotting()\n",
    "\n",
    "total_episodes = 1000\n",
    "max_steps = 400\n",
    "\n",
    "env = gym.make('Taxi-v3', render_mode=\"ansi\")\n",
    "Agent = AgentsQlearningAgent(env.observation_space.n, env.action_space.n, epsilon = 0.7, lr_rate = 0.81, gamma = 0.96)\n",
    "\n",
    "# Start\n",
    "for episode in range(1, total_episodes):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    t = 0\n",
    "    score = 0\n",
    "    while t < max_steps:\n",
    "        #clear_output(wait=True)\n",
    "        #print(\"*** Episode: \", episode)\n",
    "        #Environment.env.render()\n",
    "        #time.sleep(0.05)\n",
    "        action = Agent.choose_action(state)  \n",
    "        if action == \"random\":\n",
    "            action = env.action_space.sample()\n",
    "        state2, reward, terminated, truncated, info = env.step(action)\n",
    "        Agent.learn(state, state2, reward, action)\n",
    "\n",
    "        state = state2\n",
    "        t += 1\n",
    "        if terminated or truncated:\n",
    "            #print(\"*** Total Reward Received: \", reward)\n",
    "            #time.sleep(1)\n",
    "            Reward_plot.add_data(episode, reward)\n",
    "            break\n",
    "    \n",
    "Reward_plot.plot_reward()\n",
    "print(\"Agent Q-Table\")\n",
    "print(Agent.Q)\n",
    "print(Agent.Q.shape)\n",
    "\n",
    "with open(\"Training/TaxiQ.pkl\", 'wb') as f:\n",
    "    pickle.dump(Agent.Q, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-cinema",
   "metadata": {},
   "source": [
    "This problem already makes it difficult to follow the Q-Table anymore, since the State-Space becomes larger and the environment can't be represented by one singular gridworld anymore. Due to having different pickup and dropoff locations, more representations are needed. As can be seen from the Q-Table shape, there are 500 states to this problem, eventhough  the Grid World is a 5 x 5 Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-rhythm",
   "metadata": {},
   "source": [
    "### Using the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_reward = 0\n",
    "average_reward = 0\n",
    "\n",
    "env = gym.make('Taxi-v3', render_mode=\"ansi\")\n",
    "\n",
    "with open(\"Training/TaxiQ.pkl\", 'rb') as f:\n",
    "    Q = pickle.load(f)\n",
    "\n",
    "def choose_action(state):\n",
    "    action = np.argmax(Q[state, :])\n",
    "    return action\n",
    "\n",
    "# start\n",
    "for episode in range(1, 5):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    t = 0\n",
    "    while t < 400:\n",
    "        clear_output(wait=True)\n",
    "        print(\"*** Episode: \", episode)\n",
    "        a = env.render()\n",
    "        print(a)\n",
    "        time.sleep(0.2)\n",
    "        action = choose_action(state)\n",
    "\n",
    "        state2, reward, terminated, truncated, info = env.step(action)\n",
    "        state = state2\n",
    "        if terminated or truncated:\n",
    "            print(\"*** Total Reward Received: \", reward)\n",
    "            break\n",
    "\n",
    "    time.sleep(1)\n",
    "    os.system('clear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-eligibility",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-percentage",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## General Environment Setup and Visualisation on a Server\n",
    "To Display Gymnasium on the Pixelflux Servers a Virtual Display is needed in order for the visual output of Open Ai Gym, which can then be plotted in Matplotlib.\n",
    "\n",
    "The normal render() function uses the pyglet package, which needs RGB color inputs from a screen (or display) that is not present on the server. Therefore, you have to add a virtual screen to the server for it to record your inputs in this remote server setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the gym environment\n",
    "'''\n",
    "Example Environemnts:\n",
    "'''\n",
    "#env = gym.make('CartPole-v1',render_mode=\"rgb_array\")\n",
    "#env = gym.make('LunarLander-v2',render_mode=\"rgb_array\")\n",
    "env = gym.make('MountainCar-v0',render_mode=\"rgb_array\")\n",
    "\n",
    "#Start the virtual display\n",
    "display = Vis.start_display()\n",
    "\n",
    "#Reset the environment and render the inital state\n",
    "state = env.reset()\n",
    "plot = plt.imshow(env.render())\n",
    "\n",
    "#Episode Loop\n",
    "for j in range(400):\n",
    "    #Take a random action in the environment\n",
    "    action = env.action_space.sample()\n",
    "    state2, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    #Display the new State\n",
    "    Vis.output_frame(env.render(), display, plot, 1, j)\n",
    "    \n",
    "    #if the done flag is set the episode is terminated\n",
    "    if terminated or truncated:\n",
    "        plt.close() #Closing the Plot and creating a new one for each new Episode\n",
    "        break \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-works",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The Cart Pole Problem\n",
    "(Source: [CartPole Gymnasium](https://gymnasium.farama.org/environments/classic_control/cart_pole/))\\\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "It is highly recoomeneded that you read the paper: [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)) to understand the underlying concept of the DQN-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-concrete",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-excuse",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using Neural Networks to Estimate the Q-Function\n",
    "\n",
    "In deep Q-learning (DQN) a neural network is taken to approximate the Q-function. The neural network has as input the state representation and as output the different actions, that the agent can take. The Reward is used in training for back-propagation to update the weights and biases of the neural network to provide a better estimate of the Q-function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-forest",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DQN with Observations as Inputs\n",
    "(Source: [Reinforcement Learning: DQN w Pytorch](https://andrew-gordienko.medium.com/reinforcement-learning-dqn-w-pytorch-7c6faad3d1e) and [Cart Pole Gymnasium](https://gymnasium.farama.org/environments/classic_control/cart_pole/))\\\n",
    "As we saw in the last example it becomes more and more difficult to keep all the states stored in a Table and with it represent the optimal Q-Function. In the following Problem a Pole needs to be balanced on a Cart. The Agent can either move the Cart Left or Right Based on an Observation Vector(which represents the State) with Information about the Pole(Angel and Velocity) and the Cart(Position and Velocity). With these Observations there are a lot of different states that would need to be incorporated in a Table to solve the problem. It is easier to start estimating the Q-Function, this is done with a Neural Network with the Observations as Input and the Actions as Output.\n",
    "\n",
    "\n",
    "Here Deep Q-Learning(Q-Learning in conjunction with a Neural Network is used) the Network Dimensions are predefined in the Agents.py DQNAgent_observations class.\n",
    "\n",
    "If you do not want to display the Episode Render each time just comment out the line \n",
    "* `Vis.output_frame(env.render(mode='rgb_array'), display, plot, i, step_count)`\n",
    "\n",
    "With the `torch.save(Agent.network.state_dict(), \"Training/DQN_Observations.pth.tar\")` command the Trained Neural Network Parameters are saved in the Training Folder which can be loaded again and used to play the game to see how well our training performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring Variables\n",
    "EPISODES = 1000\n",
    "MEM_SIZE = 100000\n",
    "BATCH_SIZE = 512\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_DECAY = 0.999\n",
    "EXPLORATION_MIN = 0.001\n",
    "\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "Reward_plot = Vis.Plotting()\n",
    "\n",
    "# Starting Virtual Display for Inline Visualization\n",
    "display = Vis.start_display()\n",
    "\n",
    "# Creating Environemnt and Agent\n",
    "env = gym.make('CartPole-v1',render_mode=\"rgb_array\")\n",
    "Agent = Agents.DQNAgent_observations(EXPLORATION_MAX, EXPLORATION_DECAY, EXPLORATION_MIN, env, MEM_SIZE, BATCH_SIZE, GAMMA)\n",
    "time.sleep(1)\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "for i in range(1, EPISODES):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    score = 0\n",
    "    plot = plt.imshow(env.render())\n",
    "    step_count = 0\n",
    "    while True:\n",
    "        step_count += 1\n",
    "        \n",
    "        #Render running local\n",
    "        #env.render()\n",
    "        \n",
    "        #Running on server\n",
    "        if i % 100 == 0:\n",
    "            Vis.output_frame(env.render(), display, plot, i, step_count)        \n",
    "        \n",
    "        action = Agent.choose_action(state)\n",
    "        state_, reward, terminated, truncated, info = env.step(action)\n",
    "        state_ = np.reshape(state_, [1, observation_space])\n",
    "        \n",
    "        # Activate tweak for cartpole only.\n",
    "        reward = np.cos((np.abs(state_[0][0])/2)*np.pi/2)  # Motivating the agent to leave the cart in the center\n",
    "            \n",
    "        Agent.memory.add(state, action, reward, state_, terminated)\n",
    "        Agent.learn()\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score \n",
    "            #print(\"Episode {} Average Reward {} Best Reward {} Last Reward {}\".format(i, average_reward/i, best_reward, score))\n",
    "            Reward_plot.add_data(i, score)\n",
    "            plt.close()\n",
    "            break\n",
    "    display.clear_output(wait=True)\n",
    "    Reward_plot.plot_reward()\n",
    "    \n",
    "print('Complete')\n",
    "torch.save(Agent.network.state_dict(), \"Training/DQN_Observations.pth.tar\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-slide",
   "metadata": {},
   "source": [
    "#### Using the Training\n",
    "\n",
    "The previously saved network parameters get loaded to an agent and used to see how succesfull our training was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring Variables\n",
    "EPISODES = 10\n",
    "MEM_SIZE = 10\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_MAX = 0\n",
    "EXPLORATION_DECAY = 0\n",
    "EXPLORATION_MIN = 0\n",
    "\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "Reward_plot = Vis.Plotting()\n",
    "\n",
    "# Starting Virtual Display for Inline Visualization\n",
    "display = Vis.start_display()\n",
    "\n",
    "# Creating Environemnt and Agent\n",
    "env = gym.make('CartPole-v1',render_mode=\"rgb_array\")\n",
    "\n",
    "Agent = Agents.DQNAgent_observations(EXPLORATION_MAX, EXPLORATION_DECAY, EXPLORATION_MIN, env, MEM_SIZE, BATCH_SIZE, GAMMA)\n",
    "Agent.network.load_state_dict(torch.load( \"Training/DQN_Observations.pth.tar\"))\n",
    "\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "\n",
    "for i in range(1, EPISODES):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    score = 0\n",
    "    plot = plt.imshow(env.render())\n",
    "    step_count = 0\n",
    "    while True:\n",
    "        step_count += 1\n",
    "    #Render running local\n",
    "        #env.render()\n",
    "        \n",
    "    #Running on server\n",
    "        #Display the new State\n",
    "        Vis.output_frame(env.render(), display, plot, i, step_count)\n",
    "        \n",
    "        action = Agent.choose_action(state)\n",
    "        state_, reward, terminated, truncated, info = env.step(action)\n",
    "        state_ = np.reshape(state_, [1, observation_space])\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score \n",
    "            print(\"Episode {} Average Reward {} Best Reward {} Last Reward {}\".format(i, average_reward/i, best_reward, score))\n",
    "            Reward_plot.add_data(i, score)\n",
    "            plt.close()\n",
    "            break\n",
    "    Reward_plot.plot_reward()\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(1)\n",
    "print('Complete')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-sharing",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-teens",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DQN with Pixel Data - The Cart Pole Again\n",
    "(Source: [REINFORCEMENT LEARNING (DQN) TUTORIAL](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html))\\\n",
    "But what is to be done if the agent does not recieve a nicely defined Observations Vector from the environment? One could start using Pixel Data as a State Representation. Some functionality is added to the RL_Environments To get the pixel data from Open Ai Gym and to proccess it a bit to make it more usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b3b2d-2a22-4386-9bac-f41dc7ff7912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting Virtual Display for Inline Visualization\n",
    "display = Vis.start_display()\n",
    "\n",
    "env = RL_Environments.CartPole('CartPole-v1', display, image_observation = True)\n",
    "\n",
    "print(env.get_screen().shape)\n",
    "plt.figure()\n",
    "plt.imshow(env.get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy())\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-lotus",
   "metadata": {},
   "source": [
    "---\n",
    "### Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76f96f-9238-4dec-a938-88ea7bc29e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_reward = 0\n",
    "average_reward = 0\n",
    "Reward_plot = Vis.Plotting()\n",
    "\n",
    "# Starting Virtual Display for Inline Visualization\n",
    "display = Vis.start_display()\n",
    "\n",
    "env = RL_Environments.CartPole('CartPole-v1', display, image_observation = True)\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 1\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.996\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "num_episodes = 5000\n",
    "steps_done = 0\n",
    "\n",
    "Agent = Agents.DQNAgent_image(EPS_START, EPS_DECAY, EPS_END, env, BATCH_SIZE, GAMMA)\n",
    "\n",
    "for i in range(1, num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.env.reset()\n",
    "    plot = plt.imshow(env.env.render())\n",
    "    last_screen = env.get_screen()\n",
    "    current_screen = env.get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    score = 0\n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = Agent.select_action(state, steps_done)\n",
    "        steps_done += 1\n",
    "        _, reward, terminated, truncated, _ = env.env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = env.get_screen()\n",
    "            \n",
    "        if not (terminated or truncated):\n",
    "            next_state = current_screen # - last_screen\n",
    "            if i % 100 == 0:\n",
    "                Vis.output_frame(np.transpose(next_state[0,:,:,:], (1,2,0)), display, plot, i, t) \n",
    "        else:\n",
    "            next_state = None      \n",
    "        \n",
    "        # Store the transition in memory\n",
    "        Agent.memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        Agent.optimize_model()\n",
    "        if terminated or truncated:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            #average_reward += score \n",
    "            #print(\"Episode {} Average Reward {} Best Reward {} Last Reward {}\".format(i, int(average_reward/i), int(best_reward), int(score)))\n",
    "            Reward_plot.add_data(i, score.to(\"cpu\").numpy())\n",
    "            plt.close()\n",
    "            break\n",
    "    display.clear_output(wait=True)\n",
    "    Reward_plot.plot_reward()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i % TARGET_UPDATE == 0:\n",
    "        Agent.target_net.load_state_dict(Agent.policy_net.state_dict())\n",
    "        torch.save(Agent.policy_net.state_dict(), \"Training/DQN_Image.pth.tar\")\n",
    "        \n",
    "print('Complete')\n",
    "torch.save(Agent.policy_net.state_dict(), \"Training/DQN_Image.pth.tar\")\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-noise",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_reward = 0\n",
    "average_reward = 0\n",
    "Reward_plot = Vis.Plotting()\n",
    "\n",
    "# Starting Virtual Display for Inline Visualization\n",
    "display = Vis.start_display()\n",
    "\n",
    "env = RL_Environments.CartPole('CartPole-v1', display, image_observation = True)\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0\n",
    "EPS_END = 0\n",
    "EPS_DECAY = 0\n",
    "TARGET_UPDATE = 10\n",
    "episode_durations = []\n",
    "num_episodes = 10\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "Agent = Agents.DQNAgent_image(EPS_START, EPS_DECAY, EPS_END, env, BATCH_SIZE, GAMMA)\n",
    "Agent.policy_net.load_state_dict(torch.load( \"Training/DQN_Image.pth.tar\"))\n",
    "\n",
    "for i in range(1, num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.env.reset()\n",
    "    plot = plt.imshow(env.env.render())\n",
    "    last_screen = env.get_screen()\n",
    "    current_screen = env.get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    score = 0\n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = Agent.select_action(state, steps_done)\n",
    "        steps_done += 1\n",
    "        _, reward, terminated, truncated, _ = env.env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        Vis.output_frame(env.env.render(), display, plot, i, t)\n",
    "        \n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = env.get_screen()\n",
    "        \n",
    "        if not (terminated or truncated):\n",
    "            next_state = current_screen # - last_screen\n",
    "            if i % 100 == 0:\n",
    "                Vis.output_frame(np.transpose(next_state[0,:,:,:], (1,2,0)), display, plot, i, t) \n",
    "        else:\n",
    "            next_state = None \n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if terminated or truncated:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score \n",
    "            print(\"Episode {} Average Reward {} Best Reward {} Last Reward {}\".format(i, int(average_reward/i), int(best_reward), int(score)))\n",
    "            Reward_plot.add_data(i, score.to(\"cpu\").numpy())\n",
    "            plt.close()\n",
    "            break\n",
    "    Reward_plot.plot_reward()\n",
    "print('Complete')\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-retention",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-directive",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## NEAT\n",
    "Based on the NEAT Example here: [XOR NEAT Example](https://neat-python.readthedocs.io/en/latest/xor_example.html)\\\n",
    "In the two DQN Examples above, the Neural Network structure is chosen manually and then the Weights and Biases are trained with Q-Learning. Naturally the Question becomes: \"Which Network Structure and Network Parameters would solve the Problem optimally?\". Luckily, an Algorithm has been developed which takles exactly this question. The so called NEAT (Neuroevolution of augmenting topologies) is an algorithm which tries to solve problems by just changing the structure of multiple different Networks and seeing how they perform while evolving networks that perform well.\n",
    "\n",
    "The NEAT implemation is done with the neat-python package which already has most of the functionality needed to solve the RL problems of Open Ai Gym.\n",
    "\n",
    "A config file is used to set all the parameters. This Config File can be found in NEAT_Config. If you want to make changes to it: Open the file and change the wanted parameters then save the file. Be sure to restart the kernel, otherwise changes won't take affect.\n",
    "\n",
    "To get a more detailed view of what is happening and which genomes are evaluated uncoment the line 'print(genome)' in the eval_genomes function. This will display the genomes in the classic NEAT representation after they have been tested on the environment. At the start set the number of generation quite low so you can see what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a211ecb-69d0-4240-97e0-8b54b55c8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation Function, represents our Agent Environment Interaction previously\n",
    "def eval_genomes(genomes, config):\n",
    "    for _, genome in genomes:\n",
    "        observation = [0, 0, 0, 0]  #Inital observation\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        net = neat.nn.FeedForwardNetwork.create(genome, config) #Creat net for genome with configs\n",
    "        genome.fitness = 0  #Starting fitness of 0\n",
    "        while not (terminated or truncated):\n",
    "            #Display the new State\n",
    "            #Vis.output_frame(env.render(), display, plot, i, step_count)     \n",
    "            \n",
    "            output = net.activate(observation)  #Getting output from net based on observations\n",
    "\n",
    "            action = max(output)\n",
    "            if output[0] == action:\n",
    "                action = 1\n",
    "            else:\n",
    "                action = 0\n",
    "            \n",
    "            observation, reward, terminated, truncated, info = env.step(action) #Performs action\n",
    "            \n",
    "            # Activate tweak for cartpole only.\n",
    "            reward = np.cos((np.abs(observation[0])/2)*np.pi/2)  # Motivating the agent to leave the cart in the center\n",
    "        \n",
    "            genome.fitness += reward    #Rewards genome for each turn\n",
    "        #print(genome)\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting Virtual Display for Inline Visualization\n",
    "display = Vis.start_display()\n",
    "\n",
    "# Creating Environemnt and Agent\n",
    "env = gym.make('CartPole-v1',render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "\n",
    "#load neat config\n",
    "local_dir = os.path.dirname(\"NEAT_Config/config-feedforward.txt\")\n",
    "config_path = os.path.join(local_dir, 'config-feedforward.txt')\n",
    "config = neat.config.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                         neat.DefaultSpeciesSet, neat.DefaultStagnation, config_path)\n",
    "\n",
    "# Create the population, which is the top-level object for a NEAT run.\n",
    "p = neat.Population(config)\n",
    "\n",
    "# Add a stdout reporter to show progress in the terminal.\n",
    "p.add_reporter(neat.StdOutReporter(True))\n",
    "stats = neat.StatisticsReporter()\n",
    "p.add_reporter(stats)\n",
    "#p.add_reporter(neat.Checkpointer(5))\n",
    "\n",
    "# Run for up to x generations.\n",
    "winner = p.run(eval_genomes, 50)\n",
    "\n",
    "# show final stats\n",
    "print('\\nBest genome:\\n{!s}'.format(winner))\n",
    "\n",
    "#Save the Winner genome\n",
    "with open(\"Training/Neat_winner.pkl\", \"wb\") as f:\n",
    "    pickle.dump(winner, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-worry",
   "metadata": {},
   "source": [
    "### Using the Training\n",
    "The best Genome after the training is saved and then loaded here and used to play the Cart Pole Problem to see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load requried NEAT config\n",
    "local_dir = os.path.dirname(\"NEAT_Config/config-feedforward.txt\")\n",
    "config_path = os.path.join(local_dir, 'config-feedforward.txt')\n",
    "config = neat.config.Config(neat.DefaultGenome, \n",
    "                                neat.DefaultReproduction, neat.DefaultSpeciesSet, \n",
    "                                neat.DefaultStagnation, config_path)\n",
    "\n",
    "# Unpickle saved winner\n",
    "with open(\"Training/Neat_winner.pkl\", \"rb\") as f:\n",
    "    genome = pickle.load(f)\n",
    "\n",
    "genome_net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "\n",
    "# Starting Virtual Display for Inline Visualization\n",
    "display = Vis.start_display()\n",
    "\n",
    "# Creating Environemnt and Agent\n",
    "env = gym.make('CartPole-v1',render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "    \n",
    "observation = [0, 0, 0, 0]\n",
    "score = 0\n",
    "reward = 0\n",
    "for i in range(1, 6):\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    observation = [0, 0, 0, 0]\n",
    "    step_count = 0\n",
    "    plot = plt.imshow(env.render())\n",
    "        \n",
    "    while not (terminated or truncated):\n",
    "        step_count += 1\n",
    "        #Running on server\n",
    "        Vis.output_frame(env.render(), display, plot, i, step_count)\n",
    "            \n",
    "        output = genome_net.activate(observation)\n",
    "        action = max(output)\n",
    "        if output[0] == action:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = 0\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action) #Performs action\n",
    "        score += reward\n",
    "    plt.close()\n",
    "    env.reset()\n",
    "\n",
    "print(\"Score Over 5 tries:\")\n",
    "print(score/5)\n",
    "print('\\nBest genome:\\n{!s}'.format(genome))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-taste",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (EITB712M)",
   "language": "python",
   "name": "eitb712m_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
